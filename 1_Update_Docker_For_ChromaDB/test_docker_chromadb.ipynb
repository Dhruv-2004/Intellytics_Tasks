{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChromaDB Docker Setup — Connectivity & Smoke Tests\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Run `docker compose up -d chromadb` (or `docker compose up -d` for all services)\n",
        "2. Wait for the health check to pass: `docker compose ps`\n",
        "3. ChromaDB server should be accessible at **localhost:8001**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB Python client version: 1.0.15\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "import requests\n",
        "\n",
        "CHROMA_HOST = \"localhost\"\n",
        "CHROMA_PORT = 8001\n",
        "\n",
        "print(f\"ChromaDB Python client version: {chromadb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Raw HTTP Health Check\n",
        "Verify the ChromaDB server is reachable before using the Python client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heartbeat endpoint: 200 — {'nanosecond heartbeat': 1771479316705924690}\n",
            "Server version:     200 — 1.0.0\n"
          ]
        }
      ],
      "source": [
        "base_url = f\"http://{CHROMA_HOST}:{CHROMA_PORT}\"\n",
        "\n",
        "try:\n",
        "    resp = requests.get(f\"{base_url}/api/v2/heartbeat\", timeout=5)\n",
        "    print(f\"Heartbeat endpoint: {resp.status_code} — {resp.json()}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"ERROR: Cannot reach ChromaDB server. Is the container running?\")\n",
        "    print(\"  Run: docker compose up -d chromadb\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    resp = requests.get(f\"{base_url}/api/v2/version\", timeout=5)\n",
        "    print(f\"Server version:     {resp.status_code} — {resp.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Version endpoint not available (non-critical): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Python Client Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client heartbeat: 1771479317481408565\n",
            "Server version:   1.0.0\n",
            "\n",
            "Python client connected to ChromaDB server successfully!\n"
          ]
        }
      ],
      "source": [
        "client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\n",
        "\n",
        "heartbeat = client.heartbeat()\n",
        "print(f\"Client heartbeat: {heartbeat}\")\n",
        "\n",
        "version = client.get_version()\n",
        "print(f\"Server version:   {version}\")\n",
        "\n",
        "print(\"\\nPython client connected to ChromaDB server successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Collection CRUD & Document Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted pre-existing 'docker_smoke_test' collection\n",
            "Created collection: docker_smoke_test\n",
            "Initial count:      0\n"
          ]
        }
      ],
      "source": [
        "TEST_COLLECTION = \"docker_smoke_test\"\n",
        "\n",
        "# Clean slate\n",
        "existing = [c.name for c in client.list_collections()]\n",
        "if TEST_COLLECTION in existing:\n",
        "    client.delete_collection(TEST_COLLECTION)\n",
        "    print(f\"Deleted pre-existing '{TEST_COLLECTION}' collection\")\n",
        "\n",
        "collection = client.get_or_create_collection(name=TEST_COLLECTION)\n",
        "print(f\"Created collection: {collection.name}\")\n",
        "print(f\"Initial count:      {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents added. Collection count: 3\n"
          ]
        }
      ],
      "source": [
        "# Provide dummy embeddings to avoid ChromaDB's default ONNX embedding function,\n",
        "# which has DLL issues on some Windows setups. Real embeddings come from\n",
        "# Google/OpenAI in the LangChain integration test (cell below).\n",
        "import random\n",
        "random.seed(42)\n",
        "EMBED_DIM = 384\n",
        "\n",
        "def dummy_embedding():\n",
        "    return [random.random() for _ in range(EMBED_DIM)]\n",
        "\n",
        "collection.add(\n",
        "    ids=[\"lineage-1\", \"lineage-2\", \"lineage-3\"],\n",
        "    documents=[\n",
        "        \"source_table: orders, source_column: order_id, target_table: fact_orders, target_column: order_key, dependency_score: 0.9\",\n",
        "        \"source_table: customers, source_column: customer_id, target_table: dim_customers, target_column: customer_key, dependency_score: 0.85\",\n",
        "        \"source_table: products, source_column: product_id, target_table: dim_products, target_column: product_key, dependency_score: 0.75\",\n",
        "    ],\n",
        "    embeddings=[dummy_embedding(), dummy_embedding(), dummy_embedding()],\n",
        "    metadatas=[\n",
        "        {\"source\": \"orders\", \"target\": \"fact_orders\", \"score\": 0.9},\n",
        "        {\"source\": \"customers\", \"target\": \"dim_customers\", \"score\": 0.85},\n",
        "        {\"source\": \"products\", \"target\": \"dim_products\", \"score\": 0.75},\n",
        "    ],\n",
        ")\n",
        "print(f\"Documents added. Collection count: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (using dummy embedding vector):\n",
            "--------------------------------------------------\n",
            "  Result 1: source_table: products, source_column: product_id, target_table: dim_products, target_column: product_key, dependency_score: 0.75\n",
            "    Distance: 57.4429\n",
            "    Metadata: {'target': 'dim_products', 'score': 0.75, 'source': 'products'}\n",
            "\n",
            "  Result 2: source_table: customers, source_column: customer_id, target_table: dim_customers, target_column: customer_key, dependency_score: 0.85\n",
            "    Distance: 60.1241\n",
            "    Metadata: {'target': 'dim_customers', 'source': 'customers', 'score': 0.85}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = collection.query(\n",
        "    query_embeddings=[dummy_embedding()],\n",
        "    n_results=2,\n",
        ")\n",
        "\n",
        "print(\"Query (using dummy embedding vector):\")\n",
        "print(\"-\" * 50)\n",
        "for i, doc in enumerate(results[\"documents\"][0]):\n",
        "    print(f\"  Result {i+1}: {doc}\")\n",
        "    print(f\"    Distance: {results['distances'][0][i]:.4f}\")\n",
        "    print(f\"    Metadata: {results['metadatas'][0][i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Persistence Check\n",
        "Verify data survives collection re-fetch (server-side persistence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Persistence check passed — 3 documents retained.\n"
          ]
        }
      ],
      "source": [
        "# Re-fetch the same collection from the server\n",
        "refetched = client.get_collection(name=TEST_COLLECTION)\n",
        "assert refetched.count() == 3, f\"Expected 3 docs, got {refetched.count()}\"\n",
        "print(f\"Persistence check passed — {refetched.count()} documents retained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LangChain + ChromaDB Server Integration\n",
        "Tests the `langchain_community.vectorstores.Chroma` wrapper with an `HttpClient`, \n",
        "which is the pattern `vector_db.py` needs for Docker mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain similarity_search results:\n",
            "  - source_database: db1\n",
            "source_table: orders\n",
            "source_column: order_id\n",
            "target_table: ...\n",
            "    metadata: {'type': 'lineage', 'score': 0.85}\n",
            "  - source_database: prod_tz\n",
            "source_schema: edw_staging\n",
            "source_table: allocationrule...\n",
            "    metadata: {'score': 1.0, 'type': 'lineage'}\n",
            "\n",
            "LangChain + ChromaDB server integration test PASSED!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"GOOGLE_API_KEY not set — skipping LangChain embedding test.\")\n",
        "    print(\"Set the env var and re-run this cell to test the full pipeline.\")\n",
        "else:\n",
        "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "    embedding_fn = GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/gemini-embedding-001\",\n",
        "        google_api_key=GOOGLE_API_KEY,\n",
        "    )\n",
        "\n",
        "    LANGCHAIN_COLLECTION = \"langchain_docker_test\"\n",
        "\n",
        "    chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\n",
        "\n",
        "    db = Chroma(\n",
        "        collection_name=LANGCHAIN_COLLECTION,\n",
        "        client=chroma_client,\n",
        "        embedding_function=embedding_fn,\n",
        "    )\n",
        "\n",
        "    docs = [\n",
        "        Document(\n",
        "            page_content=\"source_database: prod_tz\\nsource_schema: edw_staging\\nsource_table: allocationrule\\nsource_column: payrollbasis\\ntarget_table: dimsharedservicesallocationrule\",\n",
        "            metadata={\"type\": \"lineage\", \"score\": 1.0},\n",
        "        ),\n",
        "        Document(\n",
        "            page_content=\"source_database: db1\\nsource_table: orders\\nsource_column: order_id\\ntarget_table: fact_orders\\ntarget_column: order_key\",\n",
        "            metadata={\"type\": \"lineage\", \"score\": 0.85},\n",
        "        ),\n",
        "    ]\n",
        "    db.add_documents(docs)\n",
        "\n",
        "    results = db.similarity_search(\"downstream impact of orders\", k=2)\n",
        "    print(\"LangChain similarity_search results:\")\n",
        "    for r in results:\n",
        "        print(f\"  - {r.page_content[:80]}...\")\n",
        "        print(f\"    metadata: {r.metadata}\")\n",
        "\n",
        "    # Cleanup\n",
        "    chroma_client.delete_collection(LANGCHAIN_COLLECTION)\n",
        "    print(\"\\nLangChain + ChromaDB server integration test PASSED!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned up 'docker_smoke_test' collection.\n",
            "Remaining collections: []\n"
          ]
        }
      ],
      "source": [
        "client.delete_collection(TEST_COLLECTION)\n",
        "print(f\"Cleaned up '{TEST_COLLECTION}' collection.\")\n",
        "\n",
        "remaining = [c.name for c in client.list_collections()]\n",
        "print(f\"Remaining collections: {remaining}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Test | Status |\n",
        "|------|--------|\n",
        "| HTTP health check | Run cells above |\n",
        "| Python client connection | Run cells above |\n",
        "| Collection CRUD | Run cells above |\n",
        "| Document add & query | Run cells above |\n",
        "| Persistence | Run cells above |\n",
        "| LangChain integration | Run cells above |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
